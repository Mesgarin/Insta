{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f25325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Name                         Profile  \\\n",
      "0  ikea  https://www.instagram.com/ikea   \n",
      "1  ikea  https://www.instagram.com/ikea   \n",
      "2  ikea  https://www.instagram.com/ikea   \n",
      "3  ikea  https://www.instagram.com/ikea   \n",
      "4  ikea  https://www.instagram.com/ikea   \n",
      "\n",
      "                                     Permalink  \\\n",
      "0     https://www.instagram.com/p/DQWZuZGkz3P/   \n",
      "1     https://www.instagram.com/p/DQuCeK8ieO3/   \n",
      "2     https://www.instagram.com/p/DQq9yeUjQxz/   \n",
      "3  https://www.instagram.com/reel/DQL8rHXgE1B/   \n",
      "4  https://www.instagram.com/reel/DP3dZNHABfm/   \n",
      "\n",
      "                                             Caption            Type  \\\n",
      "0  A cosy corner for when you need a calm moment ...           IMAGE   \n",
      "1  Smart products now simpler than ever âœ¨â â Weâ€™re ...  CAROUSEL_ALBUM   \n",
      "2  Making waste sorting a little easier âœ¨Â Say hej...  CAROUSEL_ALBUM   \n",
      "3  For everyone at the table kids grown-ups and e...            reel   \n",
      "4  Itâ€™s not about perfect plates or fancy recipes...            reel   \n",
      "\n",
      "                  Date                                         Image Link  \\\n",
      "0  2025-10-28 11:45 AM  https://imgs.socialinsider.io/instagram/2025/1...   \n",
      "1  2025-11-06 04:04 PM  https://scontent-fra5-2.cdninstagram.com/v/t51...   \n",
      "2  2025-11-05 11:25 AM  https://scontent-fra3-1.cdninstagram.com/v/t51...   \n",
      "3  2025-10-24 11:28 AM  https://scontent-fra5-1.cdninstagram.com/v/t51...   \n",
      "4  2025-10-16 12:24 PM  https://scontent-fra3-1.cdninstagram.com/v/t51...   \n",
      "\n",
      "   Engagement  Eng. Rate by Followers  Comments  Likes  Content Pillars  \\\n",
      "0        1760                0.101475        27   1733              NaN   \n",
      "1        1724                0.099399        24   1700              NaN   \n",
      "2        1250                0.072070        31   1219              NaN   \n",
      "3        1119                0.064517        23   1096              NaN   \n",
      "4         760                0.043819        19    741              NaN   \n",
      "\n",
      "         Organic Value  Estimated Reach  Estimated Views  \n",
      "0             5804.09$           168716           185587  \n",
      "1             6438.49$           189693           208663  \n",
      "2              5659.2$           168758           185634  \n",
      "3                    -           129383           142321  \n",
      "4  3206.3600000000006$            95034           104538  \n",
      "Index(['Name', 'Profile', 'Permalink', 'Caption', 'Type', 'Date', 'Image Link',\n",
      "       'Engagement', 'Eng. Rate by Followers', 'Comments', 'Likes',\n",
      "       'Content Pillars', 'Organic Value', 'Estimated Reach',\n",
      "       'Estimated Views'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø®ÙˆØ¯Øª Ø±Ø§ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†\n",
    "df = pd.read_csv(\"Data\\Posts_ikea_16_Oct_2025_14_Nov_2025_1b0e.csv\")\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0fc73c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "# Ø§Ú¯Ø± Ø¯ÙˆØ³Øª Ø¯Ø§Ø±ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ² Ùˆ Ø³Ø§Ø¹Øª Ù‡Ù… Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒ:\n",
    "df['DayOfWeek'] = df['Date'].dt.day_name()\n",
    "df['Hour'] = df['Date'].dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52ec317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nastaran\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_model = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\"   # 1 ØªØ§ 5 Ø³ØªØ§Ø±Ù‡\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d70e4939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Caption bert_sentiment_label  \\\n",
      "0  A cosy corner for when you need a calm moment ...              4 stars   \n",
      "1  Smart products now simpler than ever âœ¨â â Weâ€™re ...              5 stars   \n",
      "2  Making waste sorting a little easier âœ¨Â Say hej...              5 stars   \n",
      "3  For everyone at the table kids grown-ups and e...              5 stars   \n",
      "4  Itâ€™s not about perfect plates or fancy recipes...              3 stars   \n",
      "\n",
      "  bert_sentiment_polarity  \n",
      "0                positive  \n",
      "1                positive  \n",
      "2                positive  \n",
      "3                positive  \n",
      "4                 neutral  \n"
     ]
    }
   ],
   "source": [
    "texts = df['Caption'].fillna(\"\").astype(str).tolist()\n",
    "batch_size = 32\n",
    "\n",
    "labels = []\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    preds = sentiment_model(batch)\n",
    "    labels.extend(preds)\n",
    "\n",
    "df['bert_sentiment_label'] = [x['label'] for x in labels]\n",
    "df['bert_sentiment_score'] = [x['score'] for x in labels]\n",
    "\n",
    "# Ù†Ú¯Ø§Ø´Øª 1 ØªØ§ 5 Ø³ØªØ§Ø±Ù‡ Ø¨Ù‡ Ù…Ù†ÙÛŒ / Ø®Ù†Ø«ÛŒ / Ù…Ø«Ø¨Øª\n",
    "def stars_to_polarity(label):\n",
    "    n = int(label.split()[0])  # \"1 star\" â†’ 1\n",
    "    if n <= 2:\n",
    "        return \"negative\"\n",
    "    elif n == 3:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"positive\"\n",
    "\n",
    "df['bert_sentiment_polarity'] = df['bert_sentiment_label'].apply(stars_to_polarity)\n",
    "\n",
    "print(df[['Caption', 'bert_sentiment_label', 'bert_sentiment_polarity']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb174866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae9622eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 13:44:30,599 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9ba47a22b745359c30311b153e9a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 13:44:31,859 - BERTopic - Embedding - Completed âœ“\n",
      "2025-11-14 13:44:31,863 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-14 13:44:31,993 - BERTopic - Dimensionality - Completed âœ“\n",
      "2025-11-14 13:44:31,999 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-14 13:44:32,026 - BERTopic - Cluster - Completed âœ“\n",
      "2025-11-14 13:44:32,061 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-14 13:44:32,160 - BERTopic - Representation - Completed âœ“\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Caption  topic\n",
      "0  A cosy corner for when you need a calm moment ...     -1\n",
      "1  Smart products now simpler than ever âœ¨â â Weâ€™re ...     -1\n",
      "2  Making waste sorting a little easier âœ¨Â Say hej...     -1\n",
      "3  For everyone at the table kids grown-ups and e...     -1\n",
      "4  Itâ€™s not about perfect plates or fancy recipes...     -1\n",
      "5              Making lifelong memories (and mess) ğŸ’›     -1\n",
      "6  Its IKEA Foundation Week and were highlighting...     -1\n",
      "7  Passion makes every kitchen limitless - flavou...     -1\n",
      "8            Everyday comfort made simple ğŸ’š#BROKGLIM     -1\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# ÙÙ‚Ø· Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ú©Ù¾Ø´Ù† Ø¯Ø§Ø±Ù†Ø¯\n",
    "df_topic = df[df['Caption'].notna()].copy()\n",
    "docs = df_topic['Caption'].astype(str).tolist()\n",
    "\n",
    "n_docs = len(docs)\n",
    "print(\"Number of documents:\", n_docs)\n",
    "\n",
    "if n_docs < 3:\n",
    "    print(\"ğŸ“‰ ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø³Øªâ€ŒÙ‡Ø§ Ø®ÛŒÙ„ÛŒ Ú©Ù…Ù‡ØŒ ØªØ§Ù¾ÛŒÚ© Ù…Ø¯Ù„ÛŒÙ†Ú¯ Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± Ù†Ù…ÛŒØ´Ù‡.\")\n",
    "else:\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØªØ§Ø³Øª Ú©ÙˆÚ†Ú©: n_neighbors Ø¨Ø§ÛŒØ¯ < ØªØ¹Ø¯Ø§Ø¯ Ø³Ù†Ø¯Ù‡Ø§ Ø¨Ø§Ø´Ù‡\n",
    "    n_neighbors = max(2, min(5, n_docs - 1))   # Ø¨Ø±Ø§ÛŒ Û¹ ØªØ§ Ù¾Ø³Øª Ù…ÛŒØ´Ù‡ 5\n",
    "\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        n_components=2,\n",
    "        metric=\"cosine\",\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # HDBSCAN Ù¾ÛŒØ´â€ŒÙØ±Ø¶ min_cluster_size=10 Ø¯Ø§Ø±Ù‡ Ú©Ù‡ Ø§Ø² Û¹ Ø¨ÛŒØ´ØªØ±Ù‡!\n",
    "    # Ø¨Ø±Ø§ÛŒ Ù‡Ù…ÛŒÙ† Ø®ÙˆØ¯Ù…ÙˆÙ† Ú©ÙˆÚ†ÛŒÚ©Ø´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:\n",
    "    hdbscan_model = HDBSCAN(\n",
    "        min_cluster_size=2,\n",
    "        metric=\"euclidean\",\n",
    "        cluster_selection_method=\"eom\",\n",
    "        prediction_data=True\n",
    "    )\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        language=\"english\",\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "    df_topic['topic'] = topics\n",
    "\n",
    "    print(df_topic[['Caption', 'topic']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67d46fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 13:46:35,157 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65160f28c251494bb27a104525c3e1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 13:46:37,147 - BERTopic - Embedding - Completed âœ“\n",
      "2025-11-14 13:46:37,151 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-14 13:46:37,418 - BERTopic - Dimensionality - Completed âœ“\n",
      "2025-11-14 13:46:37,422 - BERTopic - Cluster - Start clustering the reduced embeddings\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "k must be less than or equal to the number of training points",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m topics, probs \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df_topic[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m topics\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_topic[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCaption\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\bertopic\\_bertopic.py:486\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[1;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[0;32m    482\u001b[0m         umap_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mumap_model\u001b[38;5;241m.\u001b[39mtransform(embeddings)\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(documents) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;66;03m# Cluster reduced embeddings\u001b[39;00m\n\u001b[1;32m--> 486\u001b[0m     documents, probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cluster_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mumap_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_zeroshot() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(assigned_documents) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    488\u001b[0m         documents, embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_zeroshot_topics(\n\u001b[0;32m    489\u001b[0m             documents, embeddings, assigned_documents, assigned_embeddings\n\u001b[0;32m    490\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\bertopic\\_bertopic.py:3830\u001b[0m, in \u001b[0;36mBERTopic._cluster_embeddings\u001b[1;34m(self, umap_embeddings, documents, partial_fit, y)\u001b[0m\n\u001b[0;32m   3828\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3829\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3830\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhdbscan_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mumap_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3831\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3832\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhdbscan_model\u001b[38;5;241m.\u001b[39mfit(umap_embeddings)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\hdbscan\\hdbscan_.py:1270\u001b[0m, in \u001b[0;36mHDBSCAN.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobabilities_ \u001b[38;5;241m=\u001b[39m new_probabilities\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_data:\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prediction_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch_detection_data:\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_branch_detection_data()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\hdbscan\\hdbscan_.py:1311\u001b[0m, in \u001b[0;36mHDBSCAN.generate_prediction_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1308\u001b[0m         warn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not supported for prediction data!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric))\n\u001b[0;32m   1309\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prediction_data \u001b[38;5;241m=\u001b[39m \u001b[43mPredictionData\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcondensed_tree_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtree_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtree_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_kwargs\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1320\u001b[0m     warn(\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot generate prediction data for non-vector\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspace inputs -- access to the source data rather\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthan mere distances is required!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\hdbscan\\prediction.py:103\u001b[0m, in \u001b[0;36mPredictionData.__init__\u001b[1;34m(self, data, condensed_tree, min_samples, tree_type, metric, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tree_type_map[tree_type](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_data,\n\u001b[0;32m    102\u001b[0m                                            metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcore_distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_samples\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_metric \u001b[38;5;241m=\u001b[39m DistanceMetric\u001b[38;5;241m.\u001b[39mget_metric(metric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    106\u001b[0m selected_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(condensed_tree\u001b[38;5;241m.\u001b[39m_select_clusters())\n",
      "File \u001b[1;32msklearn/neighbors/_binary_tree.pxi:1180\u001b[0m, in \u001b[0;36msklearn.neighbors._kd_tree.BinaryTree64.query\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: k must be less than or equal to the number of training points"
     ]
    }
   ],
   "source": [
    "topics, probs = topic_model.fit_transform(docs)\n",
    "df_topic['topic'] = topics\n",
    "print(df_topic[['Caption', 'topic']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fee2040e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This BERTopic instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m topic_info \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_topic_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(topic_info)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\bertopic\\_bertopic.py:1657\u001b[0m, in \u001b[0;36mBERTopic.get_topic_info\u001b[1;34m(self, topic)\u001b[0m\n\u001b[0;32m   1643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_topic_info\u001b[39m(\u001b[38;5;28mself\u001b[39m, topic: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get information about each topic including its ID, frequency, and name.\u001b[39;00m\n\u001b[0;32m   1645\u001b[0m \n\u001b[0;32m   1646\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1657\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1659\u001b[0m     info \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopic_sizes_\u001b[38;5;241m.\u001b[39mitems(), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCount\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1660\u001b[0m     info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39mTopic\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopic_labels_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\bertopic\\_utils.py:80\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(topic_model)\u001b[0m\n\u001b[0;32m     77\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m instance is not fitted yet. Call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with appropriate arguments before using this estimator.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m topic_model\u001b[38;5;241m.\u001b[39mtopics_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(topic_model)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mValueError\u001b[0m: This BERTopic instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfeb7e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nastaran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Nastaran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nastaran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'topic_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwords[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwords[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m topic_titles \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tid \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtopic_info\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist():\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tid \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'topic_info' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_keywords(words):\n",
    "    cleaned = []\n",
    "    for w in words:\n",
    "        w_clean = re.sub(r'[^a-zA-Z]', '', w).lower()\n",
    "        if len(w_clean) < 3:\n",
    "            continue\n",
    "        if w_clean in STOPWORDS:\n",
    "            continue\n",
    "        cleaned.append(w_clean)\n",
    "    return cleaned\n",
    "\n",
    "def select_main_words(words):\n",
    "    tagged = pos_tag(words)\n",
    "    filtered = [w for w, tag in tagged if tag.startswith('NN') or tag.startswith('VB')]\n",
    "    return filtered if filtered else words\n",
    "\n",
    "def make_better_title(words):\n",
    "    if not words:\n",
    "        return \"Unknown Topic\"\n",
    "    words = clean_keywords(words)\n",
    "    words = select_main_words(words)\n",
    "    if not words:\n",
    "        return \"Unnamed Topic\"\n",
    "    if len(words) == 1:\n",
    "        return words[0].capitalize()\n",
    "    if len(words) == 2:\n",
    "        return f\"{words[0].capitalize()} & {words[1].capitalize()}\"\n",
    "    return f\"{words[0].capitalize()} {words[1].capitalize()}\"\n",
    "\n",
    "topic_titles = {}\n",
    "for tid in topic_info['Topic'].tolist():\n",
    "    if tid == -1:\n",
    "        continue\n",
    "    words_raw = topic_model.get_topic(tid)\n",
    "    if words_raw is None:\n",
    "        continue\n",
    "    top_words = [w[0] for w in words_raw[:10]]\n",
    "    topic_titles[tid] = make_better_title(top_words)\n",
    "\n",
    "print(\"Sample titles:\")\n",
    "for k, v in list(topic_titles.items())[:10]:\n",
    "    print(k, \":\", v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ù…Ø·Ù…Ø¦Ù† Ø´Ùˆ df_topic Ù‡Ù…Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø±Ø§ Ø¯Ø§Ø±Ø¯\n",
    "df_topic = df[df['topic'].notna()].copy()\n",
    "\n",
    "# Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§\n",
    "likes_by_topic = df_topic.groupby('topic')['Likes'].mean().round(2)\n",
    "comments_by_topic = df_topic.groupby('topic')['Comments'].mean().round(2)\n",
    "engrate_by_topic = df_topic.groupby('topic')['Eng. Rate by Followers'].mean().round(4)\n",
    "\n",
    "# Ø§Ø­Ø³Ø§Ø³ ØºØ§Ù„Ø¨ Ù‡Ø± topic\n",
    "sent_df = (\n",
    "    df_topic\n",
    "    .groupby(['topic', 'bert_sentiment_polarity'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "\n",
    "dominant_sentiment = {}\n",
    "for tid in sent_df['topic'].unique():\n",
    "    temp = sent_df[sent_df['topic'] == tid]\n",
    "    dom = temp.sort_values('count', ascending=False).iloc[0]['bert_sentiment_polarity']\n",
    "    dominant_sentiment[tid] = dom\n",
    "\n",
    "# Ø³Ø§Ø®Øª Ø¬Ø¯ÙˆÙ„\n",
    "report_rows = []\n",
    "for tid in sorted(df_topic['topic'].unique()):\n",
    "    if tid == -1:\n",
    "        continue\n",
    "    report_rows.append({\n",
    "        \"Topic ID\": tid,\n",
    "        \"Title\": topic_titles.get(tid, \"\"),\n",
    "        \"Keywords\": \", \".join([w[0] for w in (topic_model.get_topic(tid) or [])[:5]]),\n",
    "        \"Avg Likes\": likes_by_topic.get(tid, 0),\n",
    "        \"Avg Comments\": comments_by_topic.get(tid, 0),\n",
    "        \"Avg Eng. Rate\": engrate_by_topic.get(tid, 0),\n",
    "        \"Dominant Sentiment\": dominant_sentiment.get(tid, \"\")\n",
    "    })\n",
    "\n",
    "topic_report = pd.DataFrame(report_rows).sort_values(\"Avg Likes\", ascending=False)\n",
    "print(topic_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "\n",
    "import re\n",
    "\n",
    "def caption_to_hashtags(text, top_n=5):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        text,\n",
    "        keyphrase_ngram_range=(1, 2),\n",
    "        stop_words='english',\n",
    "        top_n=top_n\n",
    "    )\n",
    "    tags = []\n",
    "    for kw, score in keywords:\n",
    "        kw_clean = re.sub(r'[^a-zA-Z0-9\\s]', '', kw)\n",
    "        if not kw_clean:\n",
    "            continue\n",
    "        tag = \"#\" + kw_clean.replace(\" \", \"\").lower()\n",
    "        tags.append(tag)\n",
    "    # ÛŒÚ©ØªØ§\n",
    "    tags = list(dict.fromkeys(tags))\n",
    "    return \" \".join(tags)\n",
    "\n",
    "df['suggested_hashtags'] = df['Caption'].apply(caption_to_hashtags)\n",
    "\n",
    "print(df[['Caption', 'suggested_hashtags']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744a35af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby('Type')['Engagement'].mean().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc40e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby('Content Pillars')['Eng. Rate by Followers'].mean().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd890c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby('bert_sentiment_polarity')['Likes'].mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
